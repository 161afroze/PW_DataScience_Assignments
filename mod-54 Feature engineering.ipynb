{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b572b369",
   "metadata": {},
   "source": [
    "## 1) What are missing values in a dataset? Why is it essential to handle missing values? Name somealgorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e442ad",
   "metadata": {},
   "source": [
    "missing values in the dataset refers to the values that are absent in the dataset , they appear in the form of None or np.nan\n",
    "\n",
    "it is very essential to handle the missing value as it decreases the bias , effects the model peformance , effects the data integrity , and sometimes leads to logical erros in certain scenarios \n",
    "\n",
    "the alogorithms that are not effected by missing values are : tree based algorithms, KNN algorithm , Association rule learning( aprioiri algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84664b9",
   "metadata": {},
   "source": [
    "## 2)List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ceb394",
   "metadata": {},
   "source": [
    "If there are way too many missing values in a column then you can drop that column. \n",
    "Otherwise we can impute missing values with mean, median and mode. \n",
    "Some functions that can be used in pandas for handling missing values are the fillna, dropna, bfill and interpolate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bb2fb3",
   "metadata": {},
   "source": [
    "## 3) What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-sampling are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed777b16",
   "metadata": {},
   "source": [
    "upsampling : refers to incresing the number of datapoints in the minority class \n",
    "\n",
    "downsampling: refers to decreasing the number of datapoints in the majority class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef11f02",
   "metadata": {},
   "source": [
    "## 4) What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b211034",
   "metadata": {},
   "source": [
    "Data augmentation is a technique commonly used in machine learning and computer vision to artificially increase the size of a dataset by creating modified versions of existing data points. This is particularly useful when the available dataset is limited or when a model requires more diverse examples to learn from.\n",
    "\n",
    "\n",
    "SMOTE stands for synthetic minority oversampling techique , it uses data interploation technique to increase the number of datapoints of a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d136c5",
   "metadata": {},
   "source": [
    "## 5) What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127b26ee",
   "metadata": {},
   "source": [
    "Outliers are data points that significantly deviate from the rest of the data in a dataset.\n",
    "\n",
    "Overall, handling outliers is essential for ensuring the accuracy, reliability, and validity of data analysis and modeling efforts. It helps improve the quality of insights derived from the data and enhances the effectiveness of decision-making processes. Various techniques, such as outlier detection, transformation, and removal, can be employed to identify and address outliers appropriately, depending on the specific characteristics of the dataset and the analytical objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8784ba56",
   "metadata": {},
   "source": [
    "## 6) You are working on a project that requires analyzing customer data. However, you notice that some ofthe data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271925b8",
   "metadata": {},
   "source": [
    "we can consider the mean median mode imputations if neccessary \n",
    "\n",
    "can replace the elements with backward or forward filling\n",
    "\n",
    "can delete few elements that does not impact on the data\n",
    "\n",
    "rather we can create a computational formual to replace any missing data \n",
    "\n",
    "and finally ask domain expert to fill the values with appropriate considerable values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82d5da4",
   "metadata": {},
   "source": [
    "## 7)  You are working with a large dataset and find that a small percentage of the data is missing. What aresome strategies you can use to determine if the missing data is missing at random or if there is a patternto the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00eaf51",
   "metadata": {},
   "source": [
    "To determine if missing data in a large dataset is random or exhibits a pattern, several strategies can be employed. Visualizing missing data using techniques like heatmaps provides a quick overview, highlighting any discernible patterns. Comparing summary statistics between variables with and without missing values helps identify systematic differences, indicating potential patterns. Analyzing missingness within subgroups based on categorical variables can unveil variations, suggesting non-random missingness related to specific factors. Examining correlations between variables with missing data and other variables in the dataset can reveal associations, potentially indicating patterns. Utilizing model-based approaches to predict missing values and assessing the model's performance can indicate whether missingness is systematic. Investigating the data collection process for biases or errors can provide insights into the nature of missing data. Finally, conducting statistical tests such as Little's MCAR, MAR, or MNAR tests can formally assess the randomness of missingness. By employing these strategies, analysts can gain insights into the underlying patterns of missing data in large datasets, enabling informed decisions on how to handle them effectively in subsequent analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de5b67f",
   "metadata": {},
   "source": [
    "## 8) Suppose you are working on a medical diagnosis project and find that the majority of patients in thedataset do not have the condition of interest, while a small percentage do. What are some strategies youcan use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e74a9c0",
   "metadata": {},
   "source": [
    "When dealing with an imbalanced medical diagnosis dataset, where the majority of patients don't have the condition of interest, and only a small percentage do, several strategies can help evaluate model performance:\n",
    "\n",
    "1. **Use Evaluation Metrics**: Utilize evaluation metrics that are robust to class imbalance, such as precision, recall, F1-score, or area under the receiver operating characteristic curve (AUC-ROC).\n",
    "\n",
    "2. **Resampling Techniques**: Employ resampling techniques like oversampling the minority class (e.g., SMOTE) or undersampling the majority class to balance the dataset and provide more representative training data for the model.\n",
    "\n",
    "3. **Stratified Cross-Validation**: Implement stratified cross-validation to ensure that each fold maintains the class distribution proportions of the original dataset, preventing overfitting to the majority class.\n",
    "\n",
    "4. **Algorithm Selection**: Choose algorithms that are less sensitive to class imbalance, such as decision trees, random forests, gradient boosting machines, or support vector machines, which can handle imbalanced datasets better than others.\n",
    "\n",
    "5. **Cost-Sensitive Learning**: Adjust the class weights or misclassification costs in the model to penalize misclassifications of the minority class more than the majority class, prioritizing correct classification of the rare class.\n",
    "\n",
    "6. **Ensemble Methods**: Employ ensemble methods like bagging or boosting, which combine predictions from multiple models, to improve the overall performance and robustness on imbalanced datasets.\n",
    "\n",
    "7. **Threshold Adjustment**: Adjust the classification threshold to favor sensitivity (recall) over specificity, depending on the relative importance of correctly identifying positive cases.\n",
    "\n",
    "By employing these strategies, you can effectively evaluate the performance of your machine learning model on imbalanced medical diagnosis datasets, ensuring reliable and accurate predictions, particularly for the minority class of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a977fd1",
   "metadata": {},
   "source": [
    "## 9) When attempting to estimate customer satisfaction for a project, you discover that the dataset isunbalanced, with the bulk of customers reporting being satisfied. What methods can you employ tobalance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ad8cd2",
   "metadata": {},
   "source": [
    "use :\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "resample(data,n_samples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52ff40e",
   "metadata": {},
   "source": [
    "## 10)  You discover that the dataset is unbalanced with a low percentage of occurrences while working on aproject that requires you to estimate the occurrence of a rare event. What methods can you employ tobalance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0ab1b8",
   "metadata": {},
   "source": [
    "use Smote method:\n",
    "\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112d9943",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
